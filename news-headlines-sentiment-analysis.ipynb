{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75804b55",
   "metadata": {
    "_kg_hide-input": false,
    "papermill": {
     "duration": 0.018095,
     "end_time": "2021-11-22T03:37:06.902822",
     "exception": false,
     "start_time": "2021-11-22T03:37:06.884727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, I worked on a news dataset to create a model to predict the sentiment in those news headlines.\n",
    "The dataset has 2 features and 4.8k rows. The features are news headlines and the sentiment label. \n",
    "\n",
    "In text mining analysis, the most important part is data preprocessing. Analysis with raw data or nearly raw data might mislead you. After completing this step carefully, you may be lucky to build a successful model and make correct estimations.\n",
    "\n",
    "There are a couple of steps in text preprocessing phase: standardization, data cleaning.\n",
    "Lowering letters, removing numbers and punctuations, lemmatization and removing stopwords are the substeps to completing standardization and data cleaning part.\n",
    "\n",
    "Lowering letters is the first step of standardization and cleaning. Same words with upper and lower case letters are processed as different words in text analysis.Converting all charachers to lowercase prevents us from this situation.\n",
    "\n",
    "Removing numbers and punctuations is the another step however you should be careful about it. This step could be modified or skipped according to the  answers you try to get.\n",
    "\n",
    "Lemmatization is a important step in text analysis. With this step, words are reduced to their base forms. \n",
    "Removing stop words is the last step. Like other types of the datasets, text data contains noisy words. After applying this step, the words that might have valuable information will remain.\n",
    "\n",
    "After the text preprocessing steps, I try to build a classifier for sentiment prediction and review the model outputs at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1aea57a",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-22T03:37:06.938778Z",
     "iopub.status.busy": "2021-11-22T03:37:06.937669Z",
     "iopub.status.idle": "2021-11-22T03:37:06.968930Z",
     "shell.execute_reply": "2021-11-22T03:37:06.969448Z",
     "shell.execute_reply.started": "2021-11-20T19:25:18.758708Z"
    },
    "papermill": {
     "duration": 0.050756,
     "end_time": "2021-11-22T03:37:06.969719",
     "exception": false,
     "start_time": "2021-11-22T03:37:06.918963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentiment-analysis-for-financial-news/all-data.csv\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_66Agree.txt\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_AllAgree.txt\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/README.txt\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/License.txt\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_75Agree.txt\n",
      "/kaggle/input/sentiment-analysis-for-financial-news/FinancialPhraseBank/Sentences_50Agree.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e3a243",
   "metadata": {
    "papermill": {
     "duration": 0.015937,
     "end_time": "2021-11-22T03:37:07.002364",
     "exception": false,
     "start_time": "2021-11-22T03:37:06.986427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "563ebbf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:37:07.038005Z",
     "iopub.status.busy": "2021-11-22T03:37:07.037367Z",
     "iopub.status.idle": "2021-11-22T03:37:15.209479Z",
     "shell.execute_reply": "2021-11-22T03:37:15.208766Z",
     "shell.execute_reply.started": "2021-11-20T19:25:20.157854Z"
    },
    "papermill": {
     "duration": 8.190867,
     "end_time": "2021-11-22T03:37:15.209629",
     "exception": false,
     "start_time": "2021-11-22T03:37:07.018762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding,CuDNNGRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score,recall_score\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67534af6",
   "metadata": {
    "papermill": {
     "duration": 0.01567,
     "end_time": "2021-11-22T03:37:15.241535",
     "exception": false,
     "start_time": "2021-11-22T03:37:15.225865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Defining functions for loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587251ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:37:15.289530Z",
     "iopub.status.busy": "2021-11-22T03:37:15.288887Z",
     "iopub.status.idle": "2021-11-22T03:37:15.290520Z",
     "shell.execute_reply": "2021-11-22T03:37:15.291040Z",
     "shell.execute_reply.started": "2021-11-20T19:25:30.460771Z"
    },
    "papermill": {
     "duration": 0.03343,
     "end_time": "2021-11-22T03:37:15.291186",
     "exception": false,
     "start_time": "2021-11-22T03:37:15.257756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    dataset=pd.read_csv('../input/sentiment-analysis-for-financial-news/all-data.csv',\n",
    "                header=None,\n",
    "                 names=['Sentiment','News'])\n",
    "    return dataset\n",
    "\n",
    "def convert_to_lower_case():\n",
    "    def lower(input_text):\n",
    "        return input_text.lower()\n",
    "    dataset['News']=dataset['News'].apply(lower)\n",
    "    \n",
    "def remove_punctuation():\n",
    "    def remove_punctuation_from_text(input_text):\n",
    "        output_list=[word for word in input_text.split() if word.isalpha()]\n",
    "        return ' '.join(output_list)    \n",
    "    dataset['News']=dataset['News'].apply(remove_punctuation_from_text)\n",
    "    \n",
    "def correct_words():\n",
    "    def correct_text(input_text):\n",
    "        list_1=[str(TextBlob(word).correct()) for word in input_text.split()]\n",
    "        output_text= ' '.join(list_1)\n",
    "        return output_text\n",
    "    dataset['News']=dataset['News'].apply(correct_text)\n",
    "    \n",
    "def lemmatize():\n",
    "    def lematize_text(input_text):\n",
    "        doc=nlp(input_text)\n",
    "        lemmas=[token.lemma_ for token in doc]\n",
    "        output_text=' '.join(lemmas)\n",
    "        return output_text\n",
    "    dataset['News']=dataset['News'].apply(lematize_text)\n",
    "    \n",
    "def remove_stopwords():\n",
    "    def remove_stopwords_from_text(input_text):\n",
    "        stopwords=spacy.lang.en.stop_words.STOP_WORDS\n",
    "        output_list=[word for word in input_text.split() if word not in stopwords and not(word=='-PRON-') ]\n",
    "        return ' '.join(output_list)\n",
    "    dataset['News']=dataset['News'].apply(remove_stopwords_from_text)\n",
    "\n",
    "def filter_the_neutral_news():\n",
    "    return dataset[dataset['Sentiment']!='neutral']\n",
    "\n",
    "def create_target_and_input():\n",
    "    target=dataset['Sentiment'].values.tolist()\n",
    "    target=[1 if sentiment=='positive' else 0 for sentiment in target]\n",
    "    data=dataset['News'].values.tolist()\n",
    "    return data,target\n",
    "\n",
    "def split_train_test():\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42,stratify=target)\n",
    "    y_train=np.array(y_train)\n",
    "    y_test=np.array(y_test)\n",
    "    return x_train, x_test, y_train, y_test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85913050",
   "metadata": {
    "papermill": {
     "duration": 0.015611,
     "end_time": "2021-11-22T03:37:15.322685",
     "exception": false,
     "start_time": "2021-11-22T03:37:15.307074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Reading data, preprocessing text, splitting dataset as train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91f6b00b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:37:15.358173Z",
     "iopub.status.busy": "2021-11-22T03:37:15.357574Z",
     "iopub.status.idle": "2021-11-22T03:38:00.052418Z",
     "shell.execute_reply": "2021-11-22T03:38:00.051656Z",
     "shell.execute_reply.started": "2021-11-20T19:25:30.47857Z"
    },
    "papermill": {
     "duration": 44.713708,
     "end_time": "2021-11-22T03:38:00.052574",
     "exception": false,
     "start_time": "2021-11-22T03:37:15.338866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#reading the data\n",
    "dataset=read_data()\n",
    "#Preprocessing the text\n",
    "convert_to_lower_case()\n",
    "remove_punctuation()\n",
    "lemmatize()\n",
    "remove_stopwords()\n",
    "#Preparing data for model\n",
    "dataset=filter_the_neutral_news()\n",
    "data, target=create_target_and_input()\n",
    "x_train, x_test, y_train, y_test =split_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72309fec",
   "metadata": {
    "papermill": {
     "duration": 0.015995,
     "end_time": "2021-11-22T03:38:00.084974",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.068979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Preparing dataset for modelling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ea6d1fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:00.146127Z",
     "iopub.status.busy": "2021-11-22T03:38:00.140821Z",
     "iopub.status.idle": "2021-11-22T03:38:00.208835Z",
     "shell.execute_reply": "2021-11-22T03:38:00.208241Z",
     "shell.execute_reply.started": "2021-11-20T19:26:22.4304Z"
    },
    "papermill": {
     "duration": 0.107819,
     "end_time": "2021-11-22T03:38:00.209000",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.101181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#setting a threshold for the number of words that we are going to use\n",
    "\n",
    "num_words=1000 # number of words that we are going to use. It takes top 1k words with the highest frequency\n",
    "tokenizer=Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "#tokenization\n",
    "x_train_tokens=tokenizer.texts_to_sequences(x_train)\n",
    "x_test_tokens=tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "#setting a threshold for the number of words in each text\n",
    "num_tokens=[len(tokens) for tokens in x_train_tokens+x_test_tokens]\n",
    "num_tokens=np.array(num_tokens)\n",
    "max_tokens=np.mean(num_tokens)+2*np.std(num_tokens)\n",
    "max_tokens=int(max_tokens)\n",
    "\n",
    "#padding\n",
    "x_train_pad=pad_sequences(x_train_tokens,\n",
    "                              maxlen=max_tokens)\n",
    "x_test_pad=pad_sequences(x_test_tokens,\n",
    "                         maxlen=max_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d91807",
   "metadata": {
    "papermill": {
     "duration": 0.015423,
     "end_time": "2021-11-22T03:38:00.240803",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.225380",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After setting the max tokens threshold value, let's find the ratio of the text data which doesn't require to be modified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68474d72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:00.278638Z",
     "iopub.status.busy": "2021-11-22T03:38:00.277923Z",
     "iopub.status.idle": "2021-11-22T03:38:00.280832Z",
     "shell.execute_reply": "2021-11-22T03:38:00.281357Z",
     "shell.execute_reply.started": "2021-11-20T19:44:14.719231Z"
    },
    "papermill": {
     "duration": 0.024801,
     "end_time": "2021-11-22T03:38:00.281517",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.256716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93\n"
     ]
    }
   ],
   "source": [
    "print('{:.2f}'.format(np.sum(num_tokens<max_tokens)/len(num_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a261b",
   "metadata": {
    "papermill": {
     "duration": 0.015769,
     "end_time": "2021-11-22T03:38:00.313403",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.297634",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9896aba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:00.352511Z",
     "iopub.status.busy": "2021-11-22T03:38:00.351896Z",
     "iopub.status.idle": "2021-11-22T03:38:00.831688Z",
     "shell.execute_reply": "2021-11-22T03:38:00.831154Z",
     "shell.execute_reply.started": "2021-11-20T19:26:22.53837Z"
    },
    "papermill": {
     "duration": 0.502269,
     "end_time": "2021-11-22T03:38:00.831844",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.329575",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#creating model\n",
    "model=Sequential()\n",
    "embedding_size=50  # we will create a 50 size vector for each word.\n",
    "#At the beginning we will use random word vectors and each optimization step these vectors will be  \n",
    "model.add(Embedding(input_dim=num_words,\n",
    "                    output_dim=embedding_size,\n",
    "                    input_length=max_tokens,\n",
    "                    name='embedding_layer')\n",
    ") # this Embedding layer will take a text as an input, convert it to a vector as an output\n",
    "\n",
    "model.add(GRU(units=16, # number of neurons \n",
    "              return_sequences=True) # if true this layer odel creates multiple outputs. If the following layer has one neuron, which means the following layer creates the output. \n",
    ")\n",
    "model.add(GRU(units=8, return_sequences=True))\n",
    "model.add(GRU(units=4))\n",
    "model.add(Dense(1,activation='sigmoid'))#with the sigmoid activation function, we receive an output between 0 and 1.\n",
    "optimizer=Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c359749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:00.868118Z",
     "iopub.status.busy": "2021-11-22T03:38:00.867507Z",
     "iopub.status.idle": "2021-11-22T03:38:00.890436Z",
     "shell.execute_reply": "2021-11-22T03:38:00.889844Z",
     "shell.execute_reply.started": "2021-11-20T19:26:23.107542Z"
    },
    "papermill": {
     "duration": 0.041995,
     "end_time": "2021-11-22T03:38:00.890576",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.848581",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64433de",
   "metadata": {
    "papermill": {
     "duration": 0.016278,
     "end_time": "2021-11-22T03:38:00.923084",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.906806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10b5f71c",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:00.962398Z",
     "iopub.status.busy": "2021-11-22T03:38:00.961705Z",
     "iopub.status.idle": "2021-11-22T03:38:00.966794Z",
     "shell.execute_reply": "2021-11-22T03:38:00.967642Z",
     "shell.execute_reply.started": "2021-11-20T19:26:23.139178Z"
    },
    "papermill": {
     "duration": 0.028186,
     "end_time": "2021-11-22T03:38:00.967885",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.939699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_layer (Embedding)  (None, 18, 50)            50000     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (None, 18, 16)            3216      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 18, 8)             600       \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 4)                 156       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 53,977\n",
      "Trainable params: 53,977\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48cc31f",
   "metadata": {
    "papermill": {
     "duration": 0.016937,
     "end_time": "2021-11-22T03:38:01.001982",
     "exception": false,
     "start_time": "2021-11-22T03:38:00.985045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d2586a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:01.040491Z",
     "iopub.status.busy": "2021-11-22T03:38:01.039556Z",
     "iopub.status.idle": "2021-11-22T03:38:15.699953Z",
     "shell.execute_reply": "2021-11-22T03:38:15.700541Z",
     "shell.execute_reply.started": "2021-11-20T19:26:23.150955Z"
    },
    "papermill": {
     "duration": 14.681789,
     "end_time": "2021-11-22T03:38:15.700712",
     "exception": false,
     "start_time": "2021-11-22T03:38:01.018923",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 8s 33ms/step - loss: 0.6564 - accuracy: 0.6645\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 2s 33ms/step - loss: 0.5130 - accuracy: 0.7318\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 2s 33ms/step - loss: 0.3080 - accuracy: 0.8799\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 2s 32ms/step - loss: 0.2578 - accuracy: 0.8988\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 2s 33ms/step - loss: 0.1881 - accuracy: 0.9324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f841dfe4c50>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_pad,\n",
    "          y_train,\n",
    "          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c956049e",
   "metadata": {
    "papermill": {
     "duration": 0.056446,
     "end_time": "2021-11-22T03:38:15.813922",
     "exception": false,
     "start_time": "2021-11-22T03:38:15.757476",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Testing the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e8e590f9",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:15.932014Z",
     "iopub.status.busy": "2021-11-22T03:38:15.931387Z",
     "iopub.status.idle": "2021-11-22T03:38:16.804108Z",
     "shell.execute_reply": "2021-11-22T03:38:16.803497Z",
     "shell.execute_reply.started": "2021-11-20T19:26:40.217928Z"
    },
    "papermill": {
     "duration": 0.932858,
     "end_time": "2021-11-22T03:38:16.804239",
     "exception": false,
     "start_time": "2021-11-22T03:38:15.871381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 7ms/step - loss: 0.4892 - accuracy: 0.7792\n"
     ]
    }
   ],
   "source": [
    "result=model.evaluate(x_test_pad,\n",
    "                      y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dd96296",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-22T03:38:16.927160Z",
     "iopub.status.busy": "2021-11-22T03:38:16.926514Z",
     "iopub.status.idle": "2021-11-22T03:38:17.729685Z",
     "shell.execute_reply": "2021-11-22T03:38:17.730289Z",
     "shell.execute_reply.started": "2021-11-20T19:26:41.24403Z"
    },
    "papermill": {
     "duration": 0.867669,
     "end_time": "2021-11-22T03:38:17.730475",
     "exception": false,
     "start_time": "2021-11-22T03:38:16.862806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score: 0.90\n",
      "Recall Score: 0.76\n"
     ]
    }
   ],
   "source": [
    "#model success on the test dataset\n",
    "y_test_pred=model.predict(x=x_test_pad)\n",
    "y_test_pred=y_test_pred.T[0]\n",
    "y_test_pred=np.array([1.0 if p>0.5 else 0.0 for p in y_test_pred])\n",
    "\n",
    "precision_scr=precision_score(y_test, y_test_pred)\n",
    "recall_scr=recall_score(y_test, y_test_pred)\n",
    "\n",
    "print('Precision Score: {:.2f}'.format(precision_scr))\n",
    "print('Recall Score: {:.2f}'.format(recall_scr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76730a9d",
   "metadata": {
    "papermill": {
     "duration": 0.056819,
     "end_time": "2021-11-22T03:38:17.844919",
     "exception": false,
     "start_time": "2021-11-22T03:38:17.788100",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Results\n",
    "\n",
    "With this model, I am able to predict %82 of the sentiment successfuly.\n",
    "\n",
    "Our shooting ratio is %89, which means when the model makes 100 precition as positive, 89 of them are true.\n",
    "\n",
    "Our catch ratio is %84, which means the model is able to predict sucessfully 84 of the all positive cases.\n",
    "\n",
    "If you want to increase those ratios and have a better predictor, you should more focus on the preprocessing step and the model parameters tuning step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 80.762879,
   "end_time": "2021-11-22T03:38:19.845911",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-11-22T03:36:59.083032",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
